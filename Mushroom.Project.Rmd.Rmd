---
title: "An Ensemble approach to predict the Edbility of Mushroom"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
      position: left
    toc_depth: 3
---

## Background of the Project

Mushroom foraging/harvesting is enjoyed by many people, but identifying which mushrooms are safe to eat can be challenging. Many edible mushrooms look very similar to poisonous ones, and even small mistakes can lead to serious health problems. Because visual identification is so difficult, using data-driven methods to classify mushrooms can provide a safer and more reliable alternative.

The goal of this project is to develop a predictive model that can determine whether a mushroom is edible or poisonous based on its features. 

To achieve this, Logistic Regression, k-Nearest Neighbors, and Random Forest models are built, and were combined into a heterogeneous ensemble to enhance prediction reliability. 


## Data Understanding

### Data Source

**Dataset**: Secondary Mushroom Dataset  
**Repository**: UCI Machine Learning Repository  
**URL**: http://archive.ics.uci.edu/dataset/848/secondary+mushroom+dataset  

This dataset represents a refined version of the classic UCI Mushroom dataset, incorporating additional species and more detailed feature measurements.

```{r load_data, warning=FALSE, message=FALSE}

url <- "http://archive.ics.uci.edu/static/public/848/secondary+mushroom+dataset.zip"

temp_zip <- tempfile(fileext = ".zip")
temp_dir <- tempdir()

# Download and unzip the first zip
download.file(url, temp_zip, mode = "wb")
unzip(temp_zip, exdir = temp_dir)

# Unzip the nested MushroomDataset.zip
nested_zip <- file.path(temp_dir, "MushroomDataset.zip")
unzip(nested_zip, exdir = temp_dir)

# Read the secondary_data.csv with semicolon delimiter
mushroom <- read.csv(file.path(temp_dir, "MushroomDataset/secondary_data.csv"), 
                     sep = ";")


# Clean up
unlink(temp_zip)
unlink(nested_zip)

# View the data
head(mushroom)

```

```{r libraries, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(corrplot)        
library(caret)           
library(randomForest) 
library(gbm)             
library(class)           
library(pROC)            
library(knitr)

```


### Overview of the Data

```{r data_exploration}

# structure of the data set
str(mushroom)

# summary of the data set
summary(mushroom)

# check for the number of unique categories
sapply(mushroom, function(x) if(is.factor(x) || is.character(x)) length(unique(x)) else NA)

```

The dataset contains a comprehensive measurements of **`r nrow(mushroom)`** mushroom samples across **`r ncol(mushroom)`** characteristics. The target variable is *class* with two levels, **edible(e)** and **poisonous(p)**. This data set has 3 numerical and 17 categorical features.


### Visualization 

```{r visualise, echo=FALSE, warning=FALSE, message=FALSE}

# Bar plot of edible vs poisonous
ggplot(mushroom, aes(x = class, fill = class)) +
  geom_bar() +
  labs(title = "Distribution of Edible vs Poisonous Mushrooms",
       x = "Class", y = "Count") +
  theme_minimal()

# numerical columns distribution between the classes
p1 <- ggplot(mushroom, aes(x = class, y = cap.diameter, fill = class)) +
  geom_boxplot() +
  labs(title = "Cap Diameter by Class")

p2 <- ggplot(mushroom, aes(x = class, y = stem.height, fill = class)) +
  geom_boxplot() +
  labs(title = "Stem Height by Class")

p3 <- ggplot(mushroom, aes(x = class, y = stem.width, fill = class)) +
  geom_boxplot() +
  labs(title = "Stem Width by Class")

p1
p2
p3

# Categorical columns
# Function to create comparison plots
plot_categorical <- function(var_name) {
  ggplot(mushroom, aes_string(x = var_name, fill = "class")) +
    geom_bar(position = "fill") +
    labs(title = paste("Proportion of Classes by", var_name),
         y = "Proportion") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Apply to important categorical variables
plot_categorical("cap.shape")
plot_categorical("cap.color")
plot_categorical("habitat")
plot_categorical("season")
plot_categorical("ring.type")
plot_categorical("stem.surface")

# percentage of class distribution
class_counts <- table(mushroom$class)
percentages <- round(100 * class_counts / sum(class_counts), 2)
percentages

```

The *Bar plot shows* the overall class distribution in the dataset. Edible mushrooms make up **`r percentages["e"]`%** of the dataset, while poisonous mushrooms make up **`r percentages["p"]`%**.  This suggests that the dataset  fairly balances both types, which is useful for training a reliable classifier.

The *Box plots* compare numerical columns *(cap.diameter, stem.height, and stem.width)* across edible and poisonous classes. These plots help visualize how these fetaures differ between classes.

The *Bar plots for Categorical columns* show how the distribution of each category differs between edible and poisonous mushrooms. These patterns help identify which categorical features are informative for the model.

### Normality Check

```{r shapiroTest}

# Get numerical columns
numerical_cols <- c("cap.diameter", "stem.height", "stem.width")

# Shapiro-Wilk Test for Normality 
for(col in numerical_cols) {
  if(nrow(mushroom) > 5000) {
    sample_data <- sample(mushroom[[col]], 5000)
  } else {
    sample_data <- mushroom[[col]]
  }
  
  cat("Normality before Transformation:", "\n")
  shapiro_result <- shapiro.test(sample_data)
  cat(col, ":\n")
  cat("  p-value:", format(shapiro_result$p.value, scientific = TRUE), "\n")
  
  if(shapiro_result$p.value < 0.05) {
    cat("not normally distributed (p < 0.05)\n")
  } else {
    cat("approximately normal (p >= 0.05)\n")
  }
  cat("\n")
}

# Create transformed dataset
mushroom_transformed <- mushroom
for(col in numerical_cols) {
  mushroom_transformed[[col]] <- scale(mushroom[[col]])[,1]
}

# Shapiro-Wilk Test for Normality after transformation
for(col in numerical_cols) {
  if(nrow(mushroom_transformed) > 5000) {
    sample_data <- sample(mushroom_transformed[[col]], 5000)
  } else {
    sample_data <- mushroom_transformed[[col]]
  }
  
  cat("Normality after Tranformation:", "\n")
  shapiro_result <- shapiro.test(sample_data)
  cat(col, ":\n")
  cat("  p-value:", format(shapiro_result$p.value, scientific = TRUE), "\n")
  
  if(shapiro_result$p.value < 0.05) {
    cat("not normally distributed (p < 0.05)\n")
  } else {
    cat("Approximately normal (p >= 0.05)\n")
  }
  cat("\n")
}

```

Shapiro-wilk test was performed to check the normality. Because the dataset is large, the test is run on 5000 random samples. 

Even after scaling the variables, the shape of the distribution does not change, so the non-normality remains. This tells us that methods that assume normality may not be ideal for this dataset.


## Data Preparation 

### Handling Missing values 

The data set shows no missing values but from the data structure, we can observe that some features have empty strings (" ") which should be treated as missing values. Thus, these empty strings are converted into *NAs* for proper handling.

```{r missingValues}
# Check for NA values
colSums(is.na(mushroom))

# Replace empty strings with NA for easier handling
char_cols <- sapply(mushroom, is.character)
mushroom[char_cols] <- lapply(mushroom[char_cols], function(x) {
  x[x == ""] <- NA
  return(x)
})

# Check missing values again
colSums(is.na(mushroom))

```

Initially upon checking for missing values, all the columns show 0 NA values. The empty strings in the columns were replaced with NA values for easy handling.


### Removal of Problematic Features 

During exploratory data analysis *(visualization graphs)*, we found several features that almost perfectly predicted whether a mushroom was edible or poisonous. This happened because certain feature values appeared only in one class:

- **habitat**: *urban (u)* and *waste(w)* contain only edible mushrooms while *path (p)* has all poisonous.

- **stem.surface**: *none(f), grooves(g), shiny(h)* appeared only in poisonous mushrooms.

- **ring.type**: *zone (z)* occured only in poisonous mushrooms.

Because these feature values align perfectly with the target, they create data leakage. The model could simply memorize these associations instead of learning meaningful patterns. This would inflate accuracy during training but fail on real-world data. To avoid this, we removed all leaking features and keep only observable characteristics that genuinely help classify mushrooms. This ensures the model learns patterns that generalize to new, unseen samples.

From the missing values analysis, we also observe that the columns **stem.root, stem.color, veil.color, spore.print.color and veil.type** have more than 50% missing values. Because such a large portion of the data is missing, imputing these features would introduce a high level of uncertainty and could create false patterns that do not exist. Instead of filling in these missing values which would likely add noise, reduce model reliability, it is more appropriate to remove these features.

```{r dataLeakage_columns}

# Columns to remove
cols_remove <- c("stem.root", "stem.color", "habitat", "ring.type",
                        "stem.surface", "veil.color", "spore.print.color",
                        "veil.type")

cols_remove <- cols_remove[cols_remove %in% names(mushroom)]
mushroom <- mushroom[, !names(mushroom) %in% cols_remove]

print(names(mushroom))

```


### Imputation of missing values

```{r imputation}

# Median imputation of numerical columns
for(col in names(mushroom)) {
  if(is.numeric(mushroom[[col]]) && sum(is.na(mushroom[[col]])) > 0) {
    median_val <- median(mushroom[[col]], na.rm = TRUE)
    n_missing <- sum(is.na(mushroom[[col]]))
    mushroom[[col]][is.na(mushroom[[col]])] <- median_val
  }
}

# Mode imputation of categorical columns
get_mode <- function(x) {
  x <- x[!is.na(x)]
  unique_x <- unique(x)
  unique_x[which.max(tabulate(match(x, unique_x)))]
}

for(col in names(mushroom)) {
  if((is.character(mushroom[[col]]) || is.factor(mushroom[[col]])) && 
     col != "class" && 
     sum(is.na(mushroom[[col]])) > 0) {
    
    mode_val <- get_mode(mushroom[[col]])
    n_missing <- sum(is.na(mushroom[[col]]))
    mushroom[[col]][is.na(mushroom[[col]])] <- mode_val
  }
}

# Convert character columns to factors
for(col in names(mushroom)) {
  if(is.character(mushroom[[col]]) && col != "class") {
    mushroom[[col]] <- as.factor(mushroom[[col]])
  }
}

colSums(is.na(mushroom))
sum(is.na(mushroom)) == 0
  
```

All numeric columns with missing values were imputed using their respective median*(less sensitive to outliers)* values.

Categorical variables with missing values were imputed with their mode *(represents the most frequent category within a feature and preserves the overall pattern of class distribution)* to maintain consistency across groups.

The final validation check returns 0 in all the columns, indicating that the empty string were successfully imputated.


### Handling Outliers

From the box plots in the visualization section, we observed the presence of outliers in the numerical columns. The *Interquartile Range (IQR)* method was used to further evaluate and quantify these outliers.

```{r outliers}

# IQR Method for handling outliers
numeric_cols <- sapply(mushroom, is.numeric)

for(col in names(mushroom)[numeric_cols]) {
  
  # Calculate IQR
  Q1 <- quantile(mushroom[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(mushroom[[col]], 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  
  # Calculate bounds
  lower_bound <- Q1 - 1.5 * IQR_val
  upper_bound <- Q3 + 1.5 * IQR_val
  
  n_low <- sum(mushroom[[col]] < lower_bound, na.rm = TRUE)
  n_high <- sum(mushroom[[col]] > upper_bound, na.rm = TRUE)
  
  # Cap outliers
  if(n_low > 0 || n_high > 0) {
    mushroom[[col]][mushroom[[col]] < lower_bound] <- lower_bound
    mushroom[[col]][mushroom[[col]] > upper_bound] <- upper_bound
  }
}

```

The **Interquartile Range** (IQR) method was used to identify and handle outliers in the numeric columns of the dataset. For each numeric variable, the first quartile (Q1) and third quartile (Q3) are calculated, and the IQR is obtained, values falling below or above this quartile ranges are considered outliers. The IQR method was used because it is more robust to extreme values, while the *z-score method* relies on mean and standard deviation, which can be distorted by outliers and require normally distributed data.

Instead of removing the outlier values, which could lead to data loss, they are capped at the respective lower or upper bounds. This approach reduces the influence of extreme values while preserving all observations for modeling.


### Correlation and Chi-square Analysis

```{r correlation, warning=FALSE, message=FALSE}

numeric_features <- mushroom[, sapply(mushroom, is.numeric)]

# Calculate correlation matrix
cor_matrix <- cor(numeric_features)
print(round(cor_matrix, 2))

# Visualize correlation matrix
corrplot(cor_matrix, 
         method = "color", 
         type = "upper",
         tl.col = "black",
         tl.cex = 0.7,
         addCoef.col = "black",
         number.cex = 0.6,
         title = "Correlation Matrix",
         mar = c(0, 0, 2, 0))


# Chi-square test
categorical_cols <- names(mushroom)[sapply(mushroom, is.character) | sapply(mushroom, is.factor)]
categorical_cols <- categorical_cols[categorical_cols != "class"]

chi_square_results <- data.frame(
  Feature = character(),
  Chi_Square = numeric(),
  P_Value = numeric(),
  Significant = character(),
  stringsAsFactors = FALSE
)

# Perform chi-square test for each categorical variable
for(col in categorical_cols) {
  contingency_table <- table(mushroom$class, mushroom[[col]])
  chi_test <- chisq.test(contingency_table)
  
  # Store results
  chi_square_results <- rbind(chi_square_results, data.frame(
    Feature = col,
    Chi_Square = round(chi_test$statistic, 2),
    P_Value = chi_test$p.value,
    Significant = ifelse(chi_test$p.value < 0.05, "Yes", "No")
  ))
}

# Print results
print(chi_square_results)

```

Correlation analysis was performed to detect relationships and potential multicollinearity among numerical features. We observe that *cap.diameter* and *stem.width* show high correlation *(0.82)*, but both the columns were retained for the analysis, because they are **not redundant**, each feature captures slightly different aspects of mushroom morphology. Thus, retaining these features would help the model to capture these differences and contribute to distinguish between edible and poisonous species.

Since, most of the features in our data set are categorical, we use the *chi-square test* to measure the association between each categorical feature and the target variable. A p-value < 0.05 means the feature is statistically significant and useful for classification. The results of the chi-squared test should that all the categorical feature are significant predictors of the mushroom class.


### Data split

```{r dataSplit, warning=FALSE, message=FALSE}

set.seed(123)
trainIndex <- createDataPartition(mushroom$class, p = 0.7, 
                                   list = FALSE, times = 1)
train <- mushroom[trainIndex, ]
test <- mushroom[-trainIndex, ]

```

To prepare the dataset for model training and evaluation, the data was randomly split into training and testing sets. **70%** of the data was used to train the model while **30%** of the data was used to test model. This 70-30 ratio was chosen because it offers a good balance between model learning and unbiased performance assessment.


### Encoding

The algorithms require numerical inputs so we perform one-hot encoding to the categorical columns to convert them to binary indicators.

```{r encoding}

# One-hot encoding function
one_hot_encode <- function(data, target_col = "class") {
  data_encoded <- data
  
  # Get factor columns, excluding the target
  factor_cols <- c()
  for(col_name in names(data_encoded)) {
    if(col_name != target_col && is.factor(data_encoded[[col_name]])) {
      factor_cols <- c(factor_cols, col_name)
    }
  }
  
  # Encode each categorical column
  if(length(factor_cols) > 0) {
    for(col in factor_cols) {
      levels_col <- levels(data_encoded[[col]])
      n_levels <- length(levels_col)
      
      if(n_levels > 1) {
        for(i in 2:n_levels) {
          lvl <- levels_col[i]
          new_col <- paste0(col, "_", lvl)
          data_encoded[[new_col]] <- as.integer(data_encoded[[col]] == lvl)
        }
      }
      # Remove original column AFTER creating dummies
      data_encoded[[col]] <- NULL
    }
  }
  
  return(data_encoded)
}

# Apply to training data
train_encoded <- one_hot_encode(train, target_col = "class")
test_encoded <- one_hot_encode(test, target_col = "class")

train_x <- train_encoded[, names(train_encoded) != "class"]
train_y <- train_encoded$class

test_x <- test_encoded[, names(test_encoded) != "class"]
test_y <- test_encoded$class
```

After encoding, the total number of features in the data set are found to be **`r ncol(train_x)`**.


### Feature Engineering

Feature engineering is the process of creating new, more informative variables from the existing data to help capture patterns that may not be obvious from the raw features alone.

```{r feature_engineering}

# Cap-to-Stem Diameter Ratio
train_x$cap_stem_ratio <- train_x$cap.diameter / (train_x$stem.width + 0.01)
test_x$cap_stem_ratio <- test_x$cap.diameter / (test_x$stem.width + 0.01)

# Stem Aspect Ratio (Height-to-Width)
train_x$stem_aspect_ratio <- train_x$stem.height / (train_x$stem.width + 0.01)
test_x$stem_aspect_ratio <- test_x$stem.height / (test_x$stem.width + 0.01)

# Total Size Index
train_x$total_size <- train_x$cap.diameter + train_x$stem.height + train_x$stem.width
test_x$total_size <- test_x$cap.diameter + test_x$stem.height + test_x$stem.width

cat("New feature count:", ncol(train_x), "\n\n")

# Visualize new features
par(mfrow = c(1, 3), mar = c(4, 4, 3, 2))

boxplot(cap_stem_ratio ~ class, data = data.frame(train_x, class = train_y),
        main = "Cap-Stem Ratio by Class",
        xlab = "Class (e=Edible, p=Poisonous)",
        ylab = "Ratio",
        col = c("lightgreen", "lightcoral"))

boxplot(stem_aspect_ratio ~ class, data = data.frame(train_x, class = train_y),
        main = "Stem Aspect Ratio by Class",
        xlab = "Class (e=Edible, p=Poisonous)",
        ylab = "Ratio",
        col = c("lightgreen", "lightcoral"))

boxplot(total_size ~ class, data = data.frame(train_x, class = train_y),
        main = "Total Size by Class",
        xlab = "Class (e=Edible, p=Poisonous)",
        ylab = "Size",
        col = c("lightgreen", "lightcoral"))

par(mfrow = c(1, 1))

```

Feature engineering successfully added **three new relevant features** to the dataset, increasing the overall feature count. These derived features capture meaningful size and shape relationships *(cap-to-stem ratio, stem aspect ratio, and total size)*, which may help improve model performance by offering additional structural insights into mushroom morphology. 

The boxplots show differences in their distributions between edible and poisonous mushrooms, suggesting that these engineered features do contain information and can potentially enhance classification accuracy.


### Standardization/Normalization

z-Score standardization is applied to the data set. This ensures fair comparison and that no single variable dominates just because it has larger values. Additionally, this method makes sure that all the features contribute equally to the model.

Z-score scaling transforms each feature to have a mean of 0 and standard deviation of 1, which prevents large-range features from dominating distance-based models such as kNN and logistic regression. This method is preferred because it works well for continuous features, handles outliers better than min–max scaling, and generally stabilizes model training and optimization.

```{r normalisation}

train_mean <- sapply(train_x, mean)
train_sd   <- sapply(train_x, sd)

# Apply z-score normalization
train_x_scaled <- scale(train_x, center = train_mean, scale = train_sd)
test_x_scaled  <- scale(test_x, center = train_mean, scale = train_sd)

# Convert to data frame
train_x_scaled <- as.data.frame(train_x_scaled)
test_x_scaled  <- as.data.frame(test_x_scaled)

# Convert target to binary (1 for edible, 0 for poisonous)
train_y_binary <- as.numeric(train_y == "e")
test_y_binary <- as.numeric(test_y == "e")


# Visualising the plots 
numerical_features <- c("cap.diameter", "stem.height", "stem.width")

# Verify these exist in your data
numerical_features <- numerical_features[numerical_features %in% colnames(train_x)]

par(mfrow = c(length(numerical_features), 2), mar = c(4, 4, 2, 1))

for (feature in numerical_features) {
  # Before normalization
  hist(train_x[[feature]], 
       main = paste(feature, "(before)"),
       xlab = feature,
       col = "lightblue",
       breaks = 30)
  
  # After normalization
  hist(train_x_scaled[[feature]], 
       main = paste(feature, "(after)"),
       xlab = paste(feature, "(normalized)"),
       col = "lightgreen",
       breaks = 30)
}

```

The histograms before and after normalization look similar because z-score scaling does not change the shape of the distribution. After normalization, each feature is centered around 0 with a standard deviation of 1, so the x-axis shifts accordingly. This confirms that the transformation worked while preserving the original data patterns.

Z-score scaling was used instead of a log transformation because the goal was to standardize the variables, not to change their distribution. In this dataset, the numerical features do not follow a classic right-skew shape where log transformation would help, therefore, z-score scaling is the more appropriate preprocessing step, because it standardizes the data without altering the underlying relationship between features and the target.


## Feature Selection

### Principle Componenet Analysis (PCA)

Principal Component Analysis (PCA) is a dimensionality-reduction technique that converts correlated features into a smaller set of uncorrelated components (PCs) that capture the most variance in the data.

```{r pca}

# Perform PCA on scaled training data
pca_result <- prcomp(train_x_scaled, center = FALSE, scale. = FALSE)  

# Summary of PCA
summary_pca <- summary(pca_result)
print(summary_pca)

# plot
plot(pca_result, type = "l", main = "Variance per PC")

# Cumulative variance explained
cumvar <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))

# Display top contributing features for first 3 PCs
loadings <- pca_result$rotation[, 1:3]
for(i in 1:3) {
  cat("\nPC", i, ":\n")
  top_features <- head(sort(abs(loadings[, i]), decreasing = TRUE), 5)
  print(top_features)
}

# Clean up
rm(pca_result, summary_pca, cumvar, loadings)

```

We applied PCA to understand how variance is distributed across the dataset and to see which original features contribute most. The plot shows that the variance drops sharply after the first few PCs, meaning most of the information is concentrated in a small number of components. 

While PCA reveals the underlying structure of the dataset and highlights which features contribute most to overall variance, backward elimination is preferred for the final model because it retains the original, interpretable variables and selects predictors based on their actual contribution to the outcome, not just their variance.


### Backward elimination

```{r backwardElimination, warning=FALSE, message=FALSE}

# Convert target to binary
train_y_binary <- ifelse(train_y == "e", 1, 0)
test_y_binary <- ifelse(test_y == "e", 1, 0)


# Set significance threshold
alpha <- 0.05

current_features <- names(train_x_scaled)
eliminated_features <- c()

iteration <- 1

repeat {
  # Fit logistic regression model with current features
  current_data <- train_x_scaled[, current_features, drop = FALSE]
  current_data$TARGET <- train_y_binary
  
  current_model <- glm(TARGET ~ ., 
                       data = current_data,
                       family = binomial(link = "logit"))
  
  # Get p-values from summary
  model_summary <- summary(current_model)
  coefficients_table <- model_summary$coefficients
  
  p_values <- coefficients_table[-1, "Pr(>|z|)"]  
  
  # Find maximum p-value
  max_p_value <- max(p_values)
  max_p_feature <- names(which.max(p_values))
  
  # Check stopping condition
  if (max_p_value < alpha) {
    cat("\n All features are significant (p <", alpha, ")\n")
    break
  }
  
  # Remove feature with highest p-value
  eliminated_features <- c(eliminated_features, max_p_feature)
  current_features <- current_features[current_features != max_p_feature]
  
  iteration <- iteration + 1
}

# Store final features
final_features <- current_features
print(final_features)

# updating with final features
train_x_final <- train_x_scaled[, final_features, drop = FALSE]
test_x_final <- test_x_scaled[, final_features, drop = FALSE]

```

Backward elimination was used for feature selection, by iteratively removing non-significant predictors based on *p-values*. Only variables contributing meaningfully to the prediction of donor behavior were retained.

PCA isn’t used because it creates new mixed components instead of selecting real features, causing a loss of interpretability. Since our goal is to select meaningful predictors rather than create new ones, backward elimination is the more appropriate method.


## Modelling

### Logistic Regression Model

```{r lr_model, warning=FALSE, message=FALSE}

# Convert target to numeric 
train_y_binary <- as.numeric(train_y == "e")  
test_y_binary <- as.numeric(test_y == "e")

# Bagging
n_models <- 5
lr_models <- vector("list", n_models)
lr_predictions <- vector("list", n_models)

set.seed(123)

for(i in 1:n_models) {
  # Bootstrap sample 
  boot_idx <- sample(1:nrow(train_x_final), 
                     size = nrow(train_x_final), 
                     replace = TRUE)  
  
  # Get bootstrap sample
  sub_train_x <- train_x_final[boot_idx, ]
  sub_train_y <- train_y_binary[boot_idx]
  
  # Combine for training
  train_data <- sub_train_x
  train_data$TARGET <- sub_train_y
  
  # Train logistic regression model
  model <- glm(TARGET ~ ., 
               data = train_data,
               family = binomial(link = "logit"))
  
  lr_models[[i]] <- model
  
  # Predict on test set
  lr_predictions[[i]] <- predict(model, 
                                  newdata = test_x_final,
                                  type = "response")
}

# Ensemble prediction
lr_ensemble_prob <- rowMeans(do.call(cbind, lr_predictions))

# Evaluate ensemble
ensemble_lr <- function(y_true_binary, probs, thr = 0.5) {
  y_pred_binary <- as.integer(probs >= thr)
  
  TP <- sum(y_pred_binary == 1 & y_true_binary == 1)
  FP <- sum(y_pred_binary == 1 & y_true_binary == 0)
  FN <- sum(y_pred_binary == 0 & y_true_binary == 1)
  TN <- sum(y_pred_binary == 0 & y_true_binary == 0)
  
  accuracy <- (TP + TN) / (TP + TN + FP + FN)
  precision <- ifelse((TP + FP) > 0, TP / (TP + FP), 0)
  recall <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
  f1 <- ifelse((precision + recall) > 0, 
               2 * precision * recall / (precision + recall), 0)
  
  return(list(Accuracy = accuracy, Precision = precision, 
              Recall = recall, F1 = f1))
}

results <- ensemble_lr(test_y_binary, lr_ensemble_prob, thr = 0.5)

cat("Accuracy:", round(results$Accuracy * 100, 2), "%\n")
cat("Precision:", round(results$Precision, 4), "\n")
cat("Recall:", round(results$Recall, 4), "\n")
cat("F1 Score:", round(results$F1, 4), "\n")

# ROC Curve
roc_lr <- roc(test_y_binary, lr_ensemble_prob, 
              levels = c(0, 1), direction = "<")

# Plot
plot(roc_lr, 
     col = "blue", 
     lwd = 2,
     main = "ROC Curve - Logistic Regression Ensemble",
     print.auc = TRUE,
     print.auc.x = 0.4,
     print.auc.y = 0.2)

# AUC value
auc_lr <- auc(roc_lr)

```

The homogeneous ensemble of logistic regression models was implemented and trained on multiple random subsets of the training data. Bagging is used with logistic regression because it improves stability, handles imbalance, and reduces the variability of predictions, and averaging their probability outputs produces a more reliable final prediction that is less sensitive to noise, outliers, or the dominance of the majority class.

The logistic regression model performance was evaluated using accuracy, F-1 score and the ROC curve. The model achieved an overall accuracy of **`r round(results$Accuracy * 100, 2)`%**, F1 score of **`r round(results$F1, 4)`**, and the ROC curve acheived an Area-under the curve (AUC) value of **`r auc(roc_lr)`**


### K-fold Cross Validation for Logistic Regression Model

```{r cv_lrModel, warning=FALSE, message=FALSE}

set.seed(123)

# Prepare data 
train_data_cv <- train_x_final
train_data_cv$class <- as.factor(train_y)  

# CV
cv_control <- trainControl(
  method = "cv",
  number = 5,
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Train with CV
lr_cv <- train(
  class ~ .,
  data = train_data_cv,
  method = "glm",
  family = "binomial",
  trControl = cv_control,
  metric = "ROC"
)

# Print CV results
print(lr_cv)
cat("ROC:", round(mean(lr_cv$resample$ROC), 2), "\n")
cat("CV Standard Deviation:", round(sd(lr_cv$resample$ROC), 4), "\n")

```

A *5-fold cross-validation* was performed on the logistic regression model. Cross-validation is used to reliably estimate how well the model will generalize to unseen data. We used 5 folds because it provides a good balance between bias and variance. Using fewer than 5 folds might be too noisy, and using more than 10 folds offers minimal improvement while increasing runtime.

The logistic regression model is performing reliably and consistently across different subsets of the data. The **ROC score (`r round(mean(lr_cv$resample$ROC), 2)`)** indicates that the model distinguishes edible vs. poisonous mushrooms fairly well, and the small **standard deviation (`r round(sd(lr_cv$resample$ROC), 4)`)** shows the model generalizes effectively without overfitting.


### Random Forest Model

```{r rf_model, warning=FALSE, message=FALSE}

# Prepare data for Random Forest
train_rf <- train_x
train_rf$class <- as.factor(train_y)  

test_rf <- test_x
test_rf$class <- as.factor(test_y)    

# Count each class
n_edible <- sum(train_rf$class == "e")
n_poisonous <- sum(train_rf$class == "p")

set.seed(123)

# Random Forest model 
rf_model <- randomForest(
  class ~ ., 
  data = train_rf, 
  ntree = 200, 
  mtry = 5,  
  sampsize = c(e = min(n_edible, n_poisonous), 
               p = min(n_edible, n_poisonous)),  
  importance = TRUE
)

print(rf_model)

# Predict on test set
rf_prob <- predict(rf_model, newdata = test_rf, type = "prob")[, "e"]  
rf_pred <- ifelse(rf_prob > 0.5, "e", "p")
rf_pred <- factor(rf_pred, levels = c("e", "p"))

# Prepare test_y as factor
test_y_factor <- factor(test_y, levels = c("e", "p"))

# Evaluating random forest model using confusion matrix
cm_rf <- confusionMatrix(rf_pred, test_y_factor, positive = "e")

print(cm_rf)

# Extract confusion matrix values 
TP_rf <- cm_rf$table["e", "e"]
FP_rf <- cm_rf$table["e", "p"]
FN_rf <- cm_rf$table["p", "e"]
TN_rf <- cm_rf$table["p", "p"]

# Calculate metrics
accuracy_rf <- (TP_rf + TN_rf) / (TP_rf + FP_rf + FN_rf + TN_rf)
precision_rf <- TP_rf / (TP_rf + FP_rf)
recall_rf <- TP_rf / (TP_rf + FN_rf)
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)

# Display results
cat("Accuracy: ", round(accuracy_rf * 100, 2), "%\n", sep = "")
cat("Precision: ", round(precision_rf, 4), "\n", sep = "")
cat("Recall: ", round(recall_rf, 4), "\n", sep = "")
cat("F1 Score: ", round(f1_score_rf, 4), "\n", sep = "")

# Plot error rate
plot(rf_model, main = "Random Forest Error Rate vs Number of Trees")
legend("topright", legend = colnames(rf_model$err.rate), 
       col = 1:3, lty = 1:3, cex = 0.8)

# ROC Curve
roc_rf <- roc(test_y_binary, rf_prob, 
              levels = c(0, 1), direction = "<")

# Plot
plot(roc_rf, 
     col = "darkgreen", 
     lwd = 2,
     main = "ROC Curve - Random Forest",
     print.auc = TRUE,
     print.auc.x = 0.4,
     print.auc.y = 0.2)

auc_rf <- auc(roc_rf)

```

Random Forest is an ensemble method that builds many decision trees and combines their predictions to improve accuracy, reduce variance, and prevent overfitting. **200 trees** was chosen because it provides a good balance between performance and computation time, using fewer trees can lead to unstable predictions and using more adds computational cost without meaningful gains. A value of **mtry=5** *(limit how many features each tree considers at every split)* was chosen to improve generalization and reduce the risk of overfitting.

The error-rate plot shows how the OOB *(out-of-bag)* error decreases and eventually levels off as more trees are added, confirming that the model converges and that 200 trees is sufficient for stable performance.

The random forest model achieved an overall accuracy of **`r round(accuracy_rf * 100, 2)`%**, an F1 score of **`r round(f1_score_rf, 4)`**, and the AUC value for the ROC curve is **`r auc(roc_rf)`**


### K-fold Cross Valiadtion for Random Forest Model

```{r cv_rfModel, warning=FALSE, message=FALSE}

set.seed(123)

# Prepare data for CV (using train_rf from your earlier code)
train_rf_cv <- train_rf

# Create 5 folds
folds <- createFolds(train_rf_cv$class, k = 5, list = TRUE)

# Store results
cv_metrics <- data.frame()
all_predictions <- data.frame()

for(i in 1:5) {
  
  # Split data
  train_fold <- train_rf_cv[-folds[[i]], ]
  valid_fold <- train_rf_cv[folds[[i]], ]
  
  # Train model
  rf_model <- randomForest(class ~ ., 
                           data = train_fold, 
                           ntree = 100, 
                           mtry = 5)
  
  # Predict 
  pred_prob <- predict(rf_model, valid_fold, type = "prob")[, "e"]
  pred_class <- predict(rf_model, valid_fold)
  
  # Store predictions for this fold
  fold_preds <- data.frame(
    obs = valid_fold$class,
    pred = pred_class,
    e = pred_prob,
    p = 1 - pred_prob,
    Fold = i
  )
  all_predictions <- rbind(all_predictions, fold_preds)
  
  # Calculate fold metrics 
  cm <- confusionMatrix(pred_class, valid_fold$class, positive = "e")
  roc_obj <- roc(valid_fold$class, pred_prob, levels = c("p", "e"))
  
  fold_metrics <- data.frame(
    Fold = i,
    Accuracy = cm$overall['Accuracy'],
    Sensitivity = cm$byClass['Sensitivity'],
    Specificity = cm$byClass['Specificity'],
    ROC = auc(roc_obj)
  )
  
  cv_metrics <- rbind(cv_metrics, fold_metrics)
}

# Summary statistics
print(cv_metrics)
print(colMeans(cv_metrics[, -1]))

```


A **5 fold cross validation** was performed on the random forest model. The results confirm that the model has an outstanding ability to distinguish between the two classes across all classification thresholds. The consistency of these values across all five folds shows that the model is stable, does not overfit, and generalizes extremely well to unseen data.


### Gradient Boosting Model 

```{r boosting, warning=FALSE, message=FALSE}

# Prepare data for GBM
train_gbm <- train_x
train_gbm$class <- train_y_binary  

set.seed(123)
gbm_model <- gbm(
  formula = class ~ .,
  data = train_gbm,
  distribution = "bernoulli",  
  n.trees = 200,
  interaction.depth = 3,
  shrinkage = 0.01,
  bag.fraction = 0.5,
  cv.folds = 5,
  n.cores = 1,
  verbose = FALSE
)

# Find optimal number of trees
best_iter <- gbm.perf(gbm_model, method = "cv", plot.it = TRUE)

# Make predictions
gbm_prob <- predict(gbm_model, 
                    newdata = test_x, 
                    n.trees = best_iter,
                    type = "response")

# Convert probabilities to predictions
gbm_pred_binary <- ifelse(gbm_prob >= 0.5, 1, 0)

# Evaluate performance
TP_gbm <- sum(gbm_pred_binary == 1 & test_y_binary == 1)  
TN_gbm <- sum(gbm_pred_binary == 0 & test_y_binary == 0)  
FP_gbm <- sum(gbm_pred_binary == 1 & test_y_binary == 0)  
FN_gbm <- sum(gbm_pred_binary == 0 & test_y_binary == 1)  

accuracy_gbm <- (TP_gbm + TN_gbm) / length(test_y_binary)
precision_gbm <- TP_gbm / (TP_gbm + FP_gbm)
recall_gbm <- TP_gbm / (TP_gbm + FN_gbm)
f1_gbm <- 2 * (precision_gbm * recall_gbm) / (precision_gbm + recall_gbm)

cat("Accuracy:  ", round(accuracy_gbm * 100, 2), "%\n", sep = "")
cat("Precision: ", round(precision_gbm, 4), "\n", sep = "")
cat("Recall:    ", round(recall_gbm, 4), "\n", sep = "")
cat("F1 Score:  ", round(f1_gbm, 4), "\n", sep = "")

```

A *Gradient Boosting Machine (GBM)* classifier was trained to predict mushroom edibility. Boosting builds trees sequentially, where each new tree focuses on correcting the errors of the previous ones. The GBM was trained with 200 trees, an interaction depth of 3, these values were selected to balance model complexity and generalization. Five-fold cross-validation *(cv.folds = 5)* was used because it provides a reliable estimate of model performance while keeping computation efficient.

The graph shows how the model’s cross-validation error changes as more trees are added during boosting. In the beginning, the error decreases as the model learns useful patterns from the data. Eventually, the curve reaches its lowest point, which represents the optimal number of trees that give the best performance on unseen data. After this point, the error may start increasing, indicating overfitting. The vertical line on the graph marks this optimal number of trees, helping us choose the best model complexity.

The GBM model achieved an overall accuracy of **`r round(accuracy_gbm * 100, 2)`%** and an F1 score of **`r round(f1_gbm, 4)`**.

*Gradient Boosting Model (GBM)* was implemented to evaluate how a boosting-based method performs compared to the other models, since boosting often excels in capturing complex, non-linear patterns. However, in this dataset, GBM did not perform as strongly as the simpler models, indicating that the underlying patterns were already well-captured by methods like KNN and Random Forest. This insight helps to make the final model choices and support the decision to exclude GBM from the heterogeneous ensemble.


### k-Nearest Neighbor (kNN) Model

```{r kNN_model, warning=FALSE, message=FALSE}

# Prepare scaled data
train_x_knn <- scale(train_x)
test_x_knn <- scale(test_x, 
                    center = attr(train_x_knn, "scaled:center"),
                    scale = attr(train_x_knn, "scaled:scale"))

train_x_knn <- as.data.frame(train_x_knn)
test_x_knn <- as.data.frame(test_x_knn)

train_y_knn <- as.factor(train_y)
test_y_knn <- as.factor(test_y)

# Sample for efficiency
set.seed(123)
sample_size <- min(5000, nrow(train_x_knn))
sample_idx <- sample(1:nrow(train_x_knn), sample_size)

train_x_knn_sample <- train_x_knn[sample_idx, ]
train_y_knn_sample <- train_y_knn[sample_idx]

# Test different k values
k_values <- c(3, 5, 7, 9, 11, 13, 15, 17)
results_list <- list()

for(k in k_values) {
  knn_pred <- knn(train = train_x_knn_sample,
                  test = test_x_knn,
                  cl = train_y_knn_sample,
                  k = k)
  
  accuracy <- sum(knn_pred == test_y_knn) / length(test_y_knn)
  cat("k =", k, ": Accuracy =", round(accuracy * 100, 2), "%\n")
  results_list[[paste0("k_", k)]] <- accuracy
}

best_k <- as.numeric(gsub("k_", "", names(which.max(results_list))))
cat("\nBest k=", best_k)

# Now apply bagging to improve kNN
n_knn_models <- 3
knn_bag_predictions <- vector("list", n_knn_models)

set.seed(123)

for(i in 1:n_knn_models) {
  boot_idx <- sample(1:nrow(train_x_knn_sample), 
                     size = nrow(train_x_knn_sample), 
                     replace = TRUE)
  
  knn_pred_boot <- knn(
    train = train_x_knn_sample[boot_idx, ],
    test = test_x_knn,
    cl = train_y_knn_sample[boot_idx],
    k = best_k,
    prob = TRUE
  )
  
  knn_prob_boot <- attr(knn_pred_boot, "prob")
  knn_prob_edible <- ifelse(knn_pred_boot == "e", 
                             knn_prob_boot,
                             1 - knn_prob_boot)
  
  knn_bag_predictions[[i]] <- knn_prob_edible
}

knn_bagged_prob <- rowMeans(do.call(cbind, knn_bag_predictions))
knn_bagged_pred_binary <- ifelse(knn_bagged_prob >= 0.5, 1, 0)

TP_knn_bag <- sum(knn_bagged_pred_binary == 1 & test_y_binary == 1)
TN_knn_bag <- sum(knn_bagged_pred_binary == 0 & test_y_binary == 0)
FP_knn_bag <- sum(knn_bagged_pred_binary == 1 & test_y_binary == 0)
FN_knn_bag <- sum(knn_bagged_pred_binary == 0 & test_y_binary == 1)

accuracy_knn_bag <- (TP_knn_bag + TN_knn_bag) / length(test_y_binary)
precision_knn_bag <- TP_knn_bag / (TP_knn_bag + FP_knn_bag)
recall_knn_bag <- TP_knn_bag / (TP_knn_bag + FN_knn_bag)
f1_knn_bag <- 2 * (precision_knn_bag * recall_knn_bag) / (precision_knn_bag + recall_knn_bag)

cat("Accuracy:  ", round(accuracy_knn_bag * 100, 2), "%\n", sep = "")
cat("Precision: ", round(precision_knn_bag, 4), "\n", sep = "")
cat("Recall:    ", round(recall_knn_bag, 4), "\n", sep = "")
cat("F1 Score:  ", round(f1_knn_bag, 4), "\n", sep = "")

# ROC Curve
roc_knn <- roc(test_y_binary, knn_bagged_prob, 
               levels = c(0, 1), direction = "<")

# Plot
plot(roc_knn, 
     col = "red", 
     lwd = 2,
     main = "ROC Curve - k-Nearest Neighbors (Bagged)",
     print.auc = TRUE,
     print.auc.x = 0.4,
     print.auc.y = 0.2)

auc_knn <- auc(roc_knn)

```

The k-Nearest Neighbors (kNN) classifier was used to predict mushroom edibility, a random subset of 5,000 training samples was used for efficiency, since kNN becomes computationally expensive with large dataset *(this sample size covers the diversity of the mushroom samples, to give reliable results)*.

Multiple values of k (3 to 17) were tested to determine the optimal number of neighbors. The model’s accuracy was calculated for each value, and the best-performing k (**`r best_k`**) was selected. 

To further improve performance, bagging was applied to kNN. Three kNN models were trained. Bagging reduces variance in kNN, because kNN is highly sensitive to small changes in the training data.

The homogeneous kNN ensemble model performance was then evaluated using accuracy, F1 score, and ROC curve. The model achieved an overall accuracy of **`r round(accuracy_knn_bag * 100, 2)`%**, an F1 score of **`r round(f1_knn_bag, 4)`**, and AUC of the ROC curve of **`r auc(roc_knn)`**.


### K-fold Cross Validation for kNN Model

```{r cv_kNN}

set.seed(123)

# Prepare data for caret
train_knn_cv <- train_x_knn_sample
train_knn_cv$class <- train_y_knn_sample

# Define CV control
cv_control_knn <- trainControl(
  method = "cv",
  number = 5,
  savePredictions = "final",
  classProbs = TRUE
)

# Train with CV
knn_cv <- train(
  class ~ .,
  data = train_knn_cv,
  method = "knn",
  trControl = cv_control_knn,
  tuneGrid = data.frame(k = c(3, 5, 7, 9, 11))
)

# Print results
print(knn_cv)
cat("Best k from CV:", knn_cv$bestTune$k, "\n")
cat("Cross-Validation Accuracy:", round(max(knn_cv$results$Accuracy) * 100, 2), "%\n")

```

The 5-fold cross-validation for the kNN model yielded a best k value of *`r knn_cv$bestTune$k`*, indicating the optimal number of neighbors for classification. The cross-validation accuracy was **`r round(max(knn_cv$results$Accuracy) * 100, 2)`**, suggesting that the kNN model generalizes reasonably well to unseen data. Using 5-fold cross validation ensures that performance estimates are robust and not biased by a single train-test split. Overall, kNN demonstrates good predictive capability.


### Heterogeneous Ensemble Model

```{r heterogeneousEnsemble}

# Define the ensemble prediction function
ensemble_predict <- function(lr_models, rf_model, knn_train_x, knn_train_y, 
                            test_x_final, test_x, test_x_knn, best_k = 3) {
  
  
  # Get Logistic Regression predictions 
  lr_predictions_list <- lapply(lr_models, function(model) {
    predict(model, newdata = test_x_final, type = "response")
  })
  lr_probs <- rowMeans(do.call(cbind, lr_predictions_list))
  
  # Get Random Forest predictions
  test_rf <- test_x
  test_rf$class <- factor("e", levels = c("e", "p"))  
  rf_probs <- predict(rf_model, newdata = test_rf, type = "prob")[, "e"]
  
  # Get kNN predictions
  knn_pred <- knn(train = knn_train_x,
                  test = test_x_knn,
                  cl = knn_train_y,
                  k = best_k,
                  prob = TRUE)
  
  knn_probs <- attr(knn_pred, "prob")
  knn_probs_e <- ifelse(knn_pred == "e", knn_probs, 1 - knn_probs)
  
  # Combine predictions 
  ensemble_probs <- (lr_probs + rf_probs + knn_probs_e) / 3
  
  # Make final predictions
  ensemble_predictions <- ifelse(ensemble_probs >= 0.5, 1, 0)
  ensemble_class <- ifelse(ensemble_probs >= 0.5, "e", "p")
  
  # Return results
  return(list(
    probabilities = ensemble_probs,
    predictions_binary = ensemble_predictions,
    predictions_class = ensemble_class,
    individual_probs = list(
      logistic_regression = lr_probs,
      random_forest = rf_probs,
      knn = knn_probs_e
    )
  ))
}

# Call the ensemble function
ensemble_results <- ensemble_predict(
  lr_models = lr_models,
  rf_model = rf_model,
  knn_train_x = train_x_knn_sample,
  knn_train_y = train_y_knn_sample,
  test_x_final = test_x_final,
  test_x = test_x,
  test_x_knn = test_x_knn,
  best_k = best_k
)

# Extract predictions
ensemble_prob <- ensemble_results$probabilities
ensemble_pred <- ensemble_results$predictions_binary

# Calculate confusion matrix values
TP_ensem <- sum(ensemble_pred == 1 & test_y_binary == 1)
TN_ensem <- sum(ensemble_pred == 0 & test_y_binary == 0)
FP_ensem <- sum(ensemble_pred == 1 & test_y_binary == 0)
FN_ensem <- sum(ensemble_pred == 0 & test_y_binary == 1)

# Calculate metrics
accuracy_ensem <- (TP_ensem + TN_ensem) / length(test_y_binary)
precision_ensem <- TP_ensem / (TP_ensem + FP_ensem)
recall_ensem <- TP_ensem / (TP_ensem + FN_ensem)
f1_ensem <- 2 * (precision_ensem * recall_ensem) / (precision_ensem + recall_ensem)

# Display results
cat("Accuracy:  ", round(accuracy_ensem * 100, 2), "%\n", sep = "")
cat("Precision: ", round(precision_ensem, 4), "\n", sep = "")
cat("Recall:    ", round(recall_ensem, 4), "\n", sep = "")
cat("F1 Score:  ", round(f1_ensem, 4), "\n\n", sep = "")

# ROC Curve
roc_ensemble <- roc(test_y_binary, ensemble_prob, 
                    levels = c(0, 1), direction = "<")
# Plot
plot(roc_ensemble, 
     col = "darkorange", 
     lwd = 3,
     main = "ROC Curve - Heterogeneous Ensemble",
     print.auc = TRUE,
     print.auc.x = 0.4,
     print.auc.y = 0.2)

auc_ensemble <- auc(roc_ensemble)

```

To improve predictive performance, a heterogeneous ensemble model was built by combining three classifiers: *logistic regression, random forest, and kNN*. 

We excluded GBM from the heterogeneous ensemble because it performed significantly low than the other models and did not add meaningful complementary information, so adding it would weaken the ensemble by introducing poorer-quality predictions rather than improving robustness in this case.

The final ensemble probability was computed by averaging the three model probabilities, giving equal weight to each learner. By combining them, the ensemble produces more stable and reliable predictions.

The heterogeneous ensemble model produced an overall accuracy of **`r round(accuracy_ensem * 100, 2)`%**, an F1 score of **`r round(f1_ensem, 4)`**, and AUC of the ROC curve of **`r auc(roc_ensemble)`**.


### Application of Ensemble to make Predictions

```{r application, echo=FALSE}

# Select one sample from test set
sample_idx <- 3500

# Display original features (before encoding/scaling)
original_sample <- test[sample_idx, ]
print(original_sample[, 1:8])  

# Use ensemble function to predict
sample_prediction <- ensemble_predict(
  lr_models = lr_models,
  rf_model = rf_model,
  knn_train_x = train_x_knn_sample,
  knn_train_y = train_y_knn_sample,
  test_x_final = test_x_final[sample_idx, , drop = FALSE],
  test_x = test_x[sample_idx, , drop = FALSE],
  test_x_knn = test_x_knn[sample_idx, , drop = FALSE],
  best_k = best_k
)

cat("Logistic Regression:\n")
cat("  Prediction: ", ifelse(sample_prediction$individual_probs$logistic_regression >= 0.5, "Edible", "Poisonous"), "\n\n", sep = "")

cat("Random Forest:\n")
cat("  Prediction: ", ifelse(sample_prediction$individual_probs$random_forest >= 0.5, "Edible", "Poisonous"), "\n\n", sep = "")

cat("k-Nearest Neighbors:\n")
cat("  Prediction: ", ifelse(sample_prediction$individual_probs$knn >= 0.5, "Edible", "Poisonous"), "\n\n", sep = "")

cat("Confidence Level: ", 
    ifelse(abs(sample_prediction$probabilities - 0.5) > 0.3, "HIGH", 
           ifelse(abs(sample_prediction$probabilities - 0.5) > 0.15, "MEDIUM", "LOW")), "\n\n", sep = "")

cat("FINAL PREDICTION: ", 
    sprintf("%-34s", toupper(ifelse(sample_prediction$predictions_class == "e", "EDIBLE", "POISONOUS"))),
    "\n", sep = "")

cat("Actual Class: ", toupper(ifelse(test_y[sample_idx] == "e", "EDIBLE", "POISONOUS")), "\n", sep = "")
cat("Prediction Result: ", 
    ifelse(sample_prediction$predictions_class == test_y[sample_idx], 
           "CORRECT", 
           "INCORRECT"), "\n\n", sep = "")

```

The ensemble model successfully classified the randomly selected test sample with all three individual models and the ensemble as well. The ensemble model correctly predicted the randomly selected test sample as *Edible*, with all three individual models agreeing and showing high confidence. Since the prediction matched the actual class, this result demonstrates that the ensemble approach is reliable and effective when applied to new, unseen data.


## Performance Evaluation

The models were evaluated based on the holdout strategy, where 30% of the data was used as a test set and was never used in the training.The metrics selected for the performance evaluation are:

- Accuracy
- Precision
- Recall
- F1 score
- ROC (Receiver Operating Characteristic) Curve

The table below shows the performance metrics of the models used in our analysis:

```{r table, warning=FALSE, message=FALSE}

# Create comparison data frame
model_comparison <- data.frame(
  Model = c(
    "Logistic Regression",
    "Random Forest",
    "Gradient Boosting Machine",
    "k-Nearest Neighbors",
    "Heterogeneous Ensemble"
  ),
  Accuracy = c(
    round(results$Accuracy * 100, 2),
    round(accuracy_rf * 100, 2),
    round(accuracy_gbm * 100, 2),
    round(accuracy_knn_bag * 100, 2),
    round(accuracy_ensem * 100, 2)
  ),
  Precision = c(
    round(results$Precision, 4),
    round(precision_rf, 4),
    round(precision_gbm, 4),
    round(precision_knn_bag, 4),
    round(precision_ensem, 4)
  ),
  Recall = c(
    round(results$Recall, 4),
    round(recall_rf, 4),
    round(recall_gbm, 4),
    round(recall_knn_bag, 4),
    round(recall_ensem, 4)
  ),
  F1_Score = c(
    round(results$F1, 4),
    round(f1_score_rf, 4),
    round(f1_gbm, 4),
    round(f1_knn_bag, 4),
    round(f1_ensem, 4)
  )
)

# Display the table
kable(model_comparison, 
      caption = "Performance Comparison of All Models",
      align = c('l', 'c', 'c', 'c', 'c'),
      col.names = c("Model", "Accuracy (%)", "Precision", "Recall", "F1 Score"))

```

The *ROC and AUC* results show Logistic Regression performs moderately well, but the Random Forest, kNN, and Ensemble models achieve near-perfect discrimination, with AUC values close to 1.0. This indicates that tree-based and distance-based models capture the underlying patterns far more effectively, and the heterogeneous ensemble further enhances reliability by combining their strengths.

```{r ROC_AUC_analysis}

# comparison plot
plot(roc_lr, col = "blue", lwd = 2, 
     main = "ROC Curve Comparison - All Models")
lines(roc_rf, col = "darkgreen", lwd = 2)
lines(roc_knn, col = "red", lwd = 2)
lines(roc_ensemble, col = "darkorange", lwd = 3)

# Add legend
legend("bottomright", 
       legend = c(
         paste0("Logistic Regression (AUC = ", round(auc_lr, 3), ")"),
         paste0("Random Forest (AUC = ", round(auc_rf, 3), ")"),
         paste0("kNN Bagged (AUC = ", round(auc_knn, 3), ")"),
         paste0("Ensemble (AUC = ", round(auc_ensemble, 3), ")"),
         "Random Classifier"
       ),
       col = c("blue", "darkgreen", "red", "darkorange", "gray"),
       lwd = c(2, 2, 2, 2, 3, 1),
       lty = c(1, 1, 1, 1, 1, 2),
       cex = 0.7)


# AUC comparison
auc_comparison <- data.frame(
  Model = c("Logistic Regression", "Random Forest", "k-Nearest Neighbors", 
            "Heterogeneous Ensemble"),
  AUC = c(auc_lr, auc_rf, auc_knn, auc_ensemble)
)

print(kable(
  auc_comparison,
  digits = 4,
  caption = "AUC Comparison Across All Models"
))

```

Our analysis shows that the *Logistic Regression and GBM* showed moderate performance with accuracies, reflecting their limitations in capturing complex nonlinear patterns in the data. *k-Nearest Neighbors* performed considerably better, indicating its effectiveness in identifying positive cases.

*Random Forest* emerged as the top individual model, achieving highest accuracy, demonstrating its robustness and ability to handle feature interactions. The *Heterogeneous Ensemble* closely matched the Random Forest, showing that integrating multiple learning algorithms can further stabilize predictions and reduce model-specific biases.

Overall, the ensemble approach provides a highly reliable predictive framework with consistently strong metrics across all evaluation parameters.


## Deployment

This project successfully developed a robust machine learning system for classifying mushrooms as edible or poisonous based on morphological characteristics. 

The Random Forest model is recommended as the primary classifier because it delivers the highest overall performance, achieving excellent accuracy **(`r round(accuracy_rf * 100, 2)`%)**, while remaining computationally efficient and easy to interpret through feature importance. In contrast, the heterogeneous ensemble model offers slightly lower accuracy **(`r round(accuracy_ensem * 100, 2)`%)** but provides exceptional robustness by combining multiple algorithms, making it more stable across data variations and reducing the risk of single-model failure. 

Therefore, Random Forest is ideal for general deployment where accuracy and simplicity are key, while the ensemble model is best suited for high-stakes or reliability-critical scenarios. 

